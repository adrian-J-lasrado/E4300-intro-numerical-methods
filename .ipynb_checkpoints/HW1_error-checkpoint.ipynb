{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "header1",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%precision 16\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Kernel $\\rightarrow$ Restart) and then run all cells (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says YOUR CODE HERE or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# HW 1 - Forms of Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q1",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Find the absolute error, relative error, and decimal precision (number of significant decimal digits) for the following $f$ and approximations $\\hat{f}$.  Note that here we may also mean precision as compared to $f$.  In these cases use the absolute error to help define $\\hat{f}$'s precision (each worth 5 points).\n",
    "\n",
    "**(a)** $f = \\pi$ and $\\hat{f} = 3.14$\n",
    "\n",
    "**(b)** $f = \\pi$ and $\\hat{f} = 22 / 7$\n",
    "\n",
    "**(c)** $f = \\log (n!)$ and $\\hat{f} = n~log(n) - n$ for $n = 5, ~~ 10, ~~ 100$ (Stirling's approximation)\n",
    "\n",
    "**(d)** $f = e^x$ and $\\hat{f} = T_n(x)$ where $T_n(x)$ is the Taylor polynomial approximation to $e^x$ expanded about $x = 0$.  Consider $N = 1, 2, 3$.  What vaule of $N$ is required for this approximation to be good to 6 digits of decimal precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A1",
     "locked": false,
     "points": 20,
     "solution": true
    }
   },
   "source": [
    "Solution:\n",
    "\n",
    "**(a)** The absolute error is $0.0015926535897930$, relative error is $0.0005069573828972$, decimal precision is $3$;\n",
    "\n",
    "**(b)** The absolute error is $0.0012644892673497$, relative error is $0.0004024994347707$, decimal precision is $3$;\n",
    "\n",
    "**(c)** For $n = 5$; The absolute error is $1.7403021806115442$, relative error is $0.3635102208239511$, decimal precision is $0$;\n",
    "\n",
    "For $n = 10$; The absolute error is $2.0785616431350551$, relative error is $0.1376128752494626$, decimal precision is $0$;\n",
    "\n",
    "For $n = 100$; The absolute error is $3.2223569567543109$, relative error is $0.0088589720368673$, decimal precision is $0$;\n",
    "\n",
    "**(d)** if $x = 0.01$\n",
    "\n",
    "When N = 1,\n",
    "\n",
    "$e^x - T_n(x) = e^x - (1 + {x}) = 0.0000501670841679$;\n",
    "\n",
    "When N = 2,\n",
    "\n",
    "$e^x - T_n(x) = e^x - (1 + {x} + \\frac{x}{2!}) = 0.0000001670841678$;\n",
    "\n",
    "Therefore, $N$ should be $2$ to be good to $6$ digits of decimal precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a)\n",
      "0.0015926535897930\n",
      "0.0005069573828972\n",
      "(b)\n",
      "0.0012644892673497\n",
      "0.0004024994347707\n",
      "(c)\n",
      "1.7403021806115442\n",
      "0.3635102208239511\n",
      "2.0785616431350551\n",
      "0.1376128752494626\n",
      "3.2223569567543109\n",
      "0.0088589720368673\n",
      "(d)\n",
      "0.0051709180756476\n",
      "0.0001709180756477\n",
      "0.0000042514089811\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print '(a)'\n",
    "print format((numpy.pi - 3.14), '.16f') \n",
    "print format(((numpy.pi - 3.14)/numpy.pi), '.16f') \n",
    "print '(b)'\n",
    "print format(abs(numpy.pi - 22./7.), '.16f') \n",
    "print format(abs((numpy.pi - 22./7.)/numpy.pi), '.16f') \n",
    "print '(c)'\n",
    "print format((numpy.log(math.factorial(5)) - (5.*numpy.log(5) - 5.)), '.16f') \n",
    "print format((numpy.log(math.factorial(5)) - (5.*numpy.log(5) - 5.))/numpy.log(math.factorial(5)), '.16f') \n",
    "print format(numpy.log(math.factorial(10)) - (10.*numpy.log(10) - 10.), '.16f') \n",
    "print format((numpy.log(math.factorial(10)) - (10.*numpy.log(10) - 10.))/numpy.log(math.factorial(10)), '.16f')\n",
    "print format((math.log(math.factorial(100)) - (100.*math.log(100) - 100.)), '.16f')\n",
    "print format((math.log(math.factorial(100)) - (100.*math.log(100) - 100.))/math.log(math.factorial(100)), '.16f')\n",
    "print '(d)'\n",
    "x = 0.1\n",
    "print format((math.exp(x) - (1 + x)), '.16f')\n",
    "print format((math.exp(x) - (1 + x + x**2/2)), '.16f')\n",
    "print format((math.exp(x) - (1 + x + x**2/math.factorial(2) + x**3/math.factorial(3))), '.16f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2-a",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**(a)** (10) Write a Python program to compute\n",
    "\n",
    "$$S_N = \\sum^N_{n=1} \\left [ \\frac{1}{n} - \\frac{1}{n+1} \\right ] = \\sum^N_{n=1} \\frac{1}{n (n + 1)}$$\n",
    "\n",
    "once using the first summation and once using the second for $N = 10, 10^2, \\ldots , 10^7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "A2-a",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sum_1(N):\n",
    "    \"\"\"Compute the summation S_N defined as\n",
    "    \n",
    "    \\sum^N_{n=1} \\left [ \\frac{1}{n} - \\frac{1}{n+1} \\right ]\n",
    "    \n",
    "    :Input:\n",
    "     *N* (int) The upper bound on the summation.\n",
    "    \n",
    "    Returns Sn (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # INSERT CODE HERE\n",
    "    Sn = 0.0\n",
    "    for n in range(1,N+1):\n",
    "        Sn += 1.0/n - 1.0/(n+1.0)\n",
    "\n",
    "    return Sn\n",
    "\n",
    "\n",
    "def sum_2(N):\n",
    "    \"\"\"Compute the summation S_N defined as\n",
    "    \n",
    "    \\sum^N_{n=1} \\frac{1}{n (n + 1)}\n",
    "    \n",
    "    :Input:\n",
    "     *N* (int) The upper bound on the summation.\n",
    "    \n",
    "    Returns Sn (float)\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT CODE HERE\n",
    "    Sn = 0.0\n",
    "    for n in range(1,N+1):\n",
    "        Sn += 1./(n*(n+1.))\n",
    "\n",
    "    return Sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "T2-a",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "N = numpy.array([10**n for n in xrange(1,8)])\n",
    "answer = numpy.zeros((2, N.shape[0]))\n",
    "for (n, upper_bound) in enumerate(N):\n",
    "    answer[0, n] = sum_1(upper_bound)\n",
    "    answer[1, n] = sum_2(upper_bound)\n",
    "numpy.testing.assert_allclose(answer[0, :], numpy.array([0.9090909090909089, 0.9900990099009896, \n",
    "                                                         0.9990009990009996, 0.9999000099990004, \n",
    "                                                         0.9999900001000117, 0.9999990000010469,\n",
    "                                                         0.9999998999998143]))\n",
    "numpy.testing.assert_allclose(answer[1, :], numpy.array([0.9090909090909091, 0.9900990099009898, \n",
    "                                                         0.9990009990009997, 0.9999000099990007, \n",
    "                                                         0.9999900001000122, 0.9999990000010476, \n",
    "                                                         0.9999998999998153]))\n",
    "print \"Success!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2-b",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**(b)**  (5) Compute the absolute error between the two summation approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "A2-b",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def abs_error(N):\n",
    "    \"\"\"Compute the absolute error of the two sums defined as\n",
    "    \n",
    "    \\sum^N_{n=1} \\left [ \\frac{1}{n} - \\frac{1}{n+1} \\right ]\n",
    "    \n",
    "    and \n",
    "    \n",
    "    \\sum^N_{n=1} \\frac{1}{n (n + 1)}\n",
    "    \n",
    "    respectively for the given N.\n",
    "    \n",
    "    :Input:\n",
    "     *N* (int) The upper bound on the summation.\n",
    "    \n",
    "    Returns *error* (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # INSERT CODE HERE\n",
    "    error = numpy.abs(sum_1(N) - sum_2(N));\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "T2-b",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "N = numpy.array([10**n for n in xrange(1,8)])\n",
    "answer = numpy.zeros(N.shape)\n",
    "for (n, upper_bound) in enumerate(N):\n",
    "    answer[n] = abs_error(upper_bound)\n",
    "numpy.testing.assert_allclose(answer, numpy.array([1.1102230246251565e-16, 1.1102230246251565e-16, \n",
    "                                                   1.1102230246251565e-16, 3.3306690738754696e-16, \n",
    "                                                   4.4408920985006262e-16, 6.6613381477509392e-16, \n",
    "                                                   9.9920072216264089e-16]))\n",
    "print \"Success!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2-c",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "**(c)** (10) Plot the relative and absolute error versus $N$.  Also plot a line where $\\epsilon_{\\text{machine}}$ should be.  Comment on what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "A2-c",
     "locked": false,
     "points": 10,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAEfCAYAAACzultCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a2b8990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 2)\n",
    "\n",
    "# HINT!  Use the plotting function semilogx to plot the errors\n",
    "#        Also, do not forget to label your plot\n",
    "\n",
    "# INSERT CODE HERE\n",
    "x = [];\n",
    "y1 = [];\n",
    "y2 = [];\n",
    "y3 = [];\n",
    "for n in range(1,8):\n",
    "    x = x + [10**(n)]\n",
    "    y1 = y1 + [abs(sum_1(10**n)-sum_2(10**n))]\n",
    "    y2 = y2 + [abs((sum_1(10**n)-sum_2(10**n))/sum_2(10**n))]\n",
    "    y3 = y3 + [numpy.finfo(float).eps]\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogx(x,y1,x,y3)\n",
    "plt.title('Relative Error versus $\\epsilon_{machine}$')\n",
    "plt.xlabel('N')\n",
    "plt.legend([\"$Relative Error$\", \"$\\epsilon_{machine}$\"], loc=2)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogx(x,y2,x,y3)\n",
    "plt.title('Absolute Error versus $\\epsilon_{machine}$')\n",
    "plt.xlabel('N')\n",
    "plt.legend([\"$Absolute Error$\", \"$\\epsilon_{machine}$\"], loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2-d",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "**(d)** (5) Theorize what may have lead to the differences in answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A2-d",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": [
    "Solution: Because the floating point arithmetic is not communitative or associative.\n",
    "\n",
    "Assume that, there are a simple arithmetic $\\delta < \\epsilon_{\\text{machine}}$\n",
    "\n",
    " $$\\frac{1}{n} - \\frac{1}{n+1} = \\frac{1}{n (n + 1)} + \\delta $$\n",
    " \n",
    " $$\\sum^N_{n=1} \\left [ \\frac{1}{n} - \\frac{1}{n+1} \\right ] = \\sum^N_{n=1} (\\frac{1}{n (n + 1)} + \\delta) = \\sum^N_{n=1} \\frac{1}{n (n + 1)} + \\sum^N_{n=1} {\\delta}$$\n",
    " \n",
    " Therefore, $$\\sum^N_{n=1} \\left [ \\frac{1}{n} - \\frac{1}{n+1} \\right ] - \\sum^N_{n=1} (\\frac{1}{n (n + 1)}) = \\sum^N_{n=1} {\\delta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Following our discussion in lecture regarding approximating $e^x$ again consider the Taylor polynomial approximation:\n",
    "\n",
    "$$e^x \\approx T_n(x) = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots + \\frac{x^n}{n!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q3-a",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "**(a)** Derive the upper bound on the *relative error* assuming that $x > 0$ and\n",
    "\n",
    "$$R_n = \\frac{|e^x - T_n(x)|}{|e^x|}$$\n",
    "\n",
    "is given by\n",
    "\n",
    "$$R_n \\leq \\left | \\frac{x^{n+1}}{(n + 1)!} \\right |$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A3-a",
     "locked": false,
     "points": 15,
     "solution": true
    }
   },
   "source": [
    "$$R_n = \\frac{|e^x - T_n(x)|}{|e^x|} = \\frac{|R_n(x)|}{|e^x|} = \\frac{f^{(n+1)}(c) \\cdot (x-x_o)^{n+1}}{(n+1)! \\cdot|e^x|} = \\frac{e^c \\cdot (x-x_o)^{n+1}}{(n+1)! \\cdot|e^x|} $$\n",
    "\n",
    "since $c < x$ and $x_0 < x$\n",
    "$$\\frac{e^c \\cdot (x-x_o)^{n+1}}{(n+1)! \\cdot|e^x|} \\leq  \\frac{x^{n+1}}{(n+1)! } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q3-b",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "**(b)** Show that for large $x$ and $n$, $r_n \\leq \\epsilon_{\\text{machine}}$ implies that we need at least $n > e \\cdot x$ terms in the series (where $e = \\text{exp}(1)$).\n",
    "\n",
    "*Hint* Use Stirling's approximation $log (n!) \\approx n~log~n - n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A3-b",
     "locked": false,
     "points": 15,
     "solution": true
    }
   },
   "source": [
    "To proof, the condition of \n",
    "$$\\frac{x^{n+1}}{(n + 1)!} <  \\epsilon_{\\text{machine}}$$ \n",
    "is $n > e \\cdot x$\n",
    "\n",
    "$$\\frac{x^{n+1}}{(n + 1)!} <  \\epsilon_{\\text{machine}}$$\n",
    "\n",
    "$${x^{n+1}} < {(n + 1)!} \\cdot \\epsilon_{\\text{machine}}$$\n",
    "\n",
    "$$(n+1) \\cdot log(x) < (n+1) \\cdot log(n+1) - (n+1) + log(\\epsilon_{\\text{machine}})$$\n",
    "\n",
    "$$log(x) < log(n+1) - 1 - \\frac{log(\\epsilon_{\\text{machine}})}{n+1}$$\n",
    "\n",
    "$$log(x) + 1 < log(n+1)$$\n",
    "\n",
    "When $x$ and $n$ are large: $log(n+1) \\approx log(n)$\n",
    "\n",
    "$$log(e \\cdot x) < log(n)$$\n",
    "\n",
    "$$n > e \\cdot x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q3-c",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**(c)** Write a Python function that accurately computes $T_n$ to the specified relative error tolerance and returns both the estimate on the range and the number of terms in the series needed over the interval $[-2, 2]$.  Note that the testing tolerance will be $8 \\cdot \\epsilon_{\\text{machine}}$.\n",
    "\n",
    "Make sure to document your code including expected inputs, outputs, and assumptions being made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "A3-c",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# HINT: Think about how we evaluated polynomials efficiently in class\n",
    "\n",
    "import scipy.misc as misc\n",
    "\n",
    "def Tn_exp(x, tolerance=1e-3):\n",
    "\n",
    "    MAX_N = 100\n",
    "    \n",
    "    # INSERT CODE HERE\n",
    "    Tn = 0\n",
    "    for N in range(MAX_N):\n",
    "        Tn = Tn + x**N/math.factorial(N)\n",
    "        if numpy.all(abs(Tn - numpy.exp(x)) < tolerance):\n",
    "            break;\n",
    "    \n",
    "    return Tn, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "T3-c",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "x = numpy.linspace(-2, 2, 100)\n",
    "tolerance = 8.0 * numpy.finfo(float).eps\n",
    "answer, N = Tn_exp(x, tolerance=tolerance)\n",
    "assert(numpy.all(numpy.abs(answer - numpy.exp(x)) / numpy.abs(numpy.exp(x)) < tolerance))\n",
    "print \"Success!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q4",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 4\n",
    "\n",
    "Given the Taylor polynomial expansions\n",
    "\n",
    "$$\\frac{1}{1-\\Delta x} = 1 + \\Delta x + \\Delta x^2 + \\Delta x^3 + \\mathcal{O}(\\Delta x^4)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\cosh \\Delta x = 1 - \\frac{\\Delta x^2}{2!} + \\frac{\\Delta x^4}{4!} + \\mathcal{O}(\\Delta x^6)$$\n",
    "\n",
    "determine the order of approximation for their sum and product (determine the exponent that belongs in the $\\mathcal{O}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A4",
     "locked": false,
     "points": 10,
     "solution": true
    }
   },
   "source": [
    "For the sum,\n",
    "$$\\frac{1}{1-\\Delta x} + \\cosh \\Delta x = 2 + \\Delta x + \\frac{\\Delta x^2 }{2} + \\Delta x^3 + \\frac{\\Delta x^4}{4!} + \\mathcal{O}(\\Delta x^4) + \\mathcal{O}(\\Delta x^6)$$\n",
    "\n",
    "since $\\mathcal{O}(\\Delta x^4) + \\frac{\\Delta x^4}{4!} = \\mathcal{O}(\\Delta x^4)$ and $\\mathcal{O}(\\Delta x^4) + \\mathcal{O}(\\Delta x^6) = \\mathcal{O}(\\Delta x^4)$.\n",
    "\n",
    "Therefore,\n",
    "$$\\frac{1}{1-\\Delta x} + \\cosh \\Delta x = 2 + \\Delta x + \\frac{\\Delta x^2 }{2} + \\Delta x^3 + \\mathcal{O}(\\Delta x^4)$$\n",
    "\n",
    "For the product,\n",
    "$$\\frac{1}{1-\\Delta x} \\cdot \\cosh \\Delta x = (1 + \\Delta x + \\Delta x^2 + \\Delta x^3 + \\mathcal{O}(\\Delta x^4)) \\cdot (1 - \\frac{\\Delta x^2}{2!} + \\frac{\\Delta x^4}{4!} + \\mathcal{O}(\\Delta x^6)) $$\n",
    "$$=  (1 + \\Delta x + \\Delta x^2 + \\Delta x^3) \\cdot (1 - \\frac{\\Delta x^2}{2!} + \\frac{\\Delta x^4}{4!}) + \\mathcal{O}(\\Delta x^{4}) \\cdot (1 - \\frac{\\Delta x^2}{2!} + \\frac{\\Delta x^4}{4!}) + (1 + \\Delta x + \\Delta x^2 + \\Delta x^3) \\cdot \\mathcal{O}(\\Delta x^6)) + \\mathcal{O}(\\Delta x^4) \\cdot \\mathcal{O}(\\Delta x^6)$$\n",
    "$$= 1 + \\Delta x + \\frac{\\Delta x^2}{2} + \\frac{\\Delta x^3}{2} - \\frac{11 \\cdot \\Delta x^4}{24} - \\frac{11 \\cdot \\Delta x^5}{24} + \\frac{\\Delta x^6}{24} + \\frac{\\Delta x^7}{24} + \\mathcal{O}(\\Delta x^{4}) + \\mathcal{O}(\\Delta x^{6}) + \\mathcal{O}(\\Delta x^{4}) \\cdot \\mathcal{O}(\\Delta x^{6})$$\n",
    "Since $\\mathcal{O}(\\Delta x^{4}) \\cdot \\mathcal{O}(\\Delta x^{6}) = \\mathcal{O}(\\Delta x^{10})$\n",
    "and\n",
    "$$ - \\frac{11 \\cdot \\Delta x^4}{24} - \\frac{11 \\cdot \\Delta x^5}{24} + \\frac{\\Delta x^6}{24} + \\frac{\\Delta x^7}{24} + \\mathcal{O}(\\Delta x^{4}) + \\mathcal{O}(\\Delta x^{6}) + \\mathcal{O}(\\Delta x^{10}) = \\mathcal{O}(\\Delta x^{4})$$\n",
    "Therefore,\n",
    "$$\\frac{1}{1-\\Delta x} \\cdot \\cosh \\Delta x = 1 + \\Delta x + \\frac{\\Delta x^2}{2} + \\frac{\\Delta x^3}{2} + \\mathcal{O}(\\Delta x^{4})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
